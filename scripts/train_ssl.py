#!/usr/bin/env python3
"""
Clean Step 4: Self-Supervised Pretraining (SSL)
- Full working training loop with MEM proxy, multi-view contrastive, jigsaw, GRL, subject-prototype alignment.
- Dotted CLI flags replaced with hyphenated ones (e.g., --data-roots).
- Safe dummy encoders wired; swap with your EFM encoders later.
"""
import os, math, time, json, random, argparse, pathlib
from dataclasses import dataclass
from typing import List, Dict, Optional

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

try:
    import yaml
except Exception:
    yaml = None


# --------------------------
# Utilities
# --------------------------
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)


class EMA:
    def __init__(self, model: nn.Module, momentum: float = 0.996):
        self.m = momentum
        self.shadow = {}
        # make initial shadows on the same device as each param
        for n, p in model.named_parameters():
            if p.requires_grad:
                self.shadow[n] = p.data.clone().detach().to(p.device)

    @torch.no_grad()
    def update(self, model: nn.Module):
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                # ensure the shadow lives on the same device as the param
                if self.shadow[n].device != p.device:
                    self.shadow[n] = self.shadow[n].to(p.device)
                self.shadow[n].mul_(self.m).add_(p.data, alpha=1.0 - self.m)

    @torch.no_grad()
    def copy_to(self, model: nn.Module):
        for n, p in model.named_parameters():
            if p.requires_grad and n in self.shadow:
                if self.shadow[n].device != p.device:
                    self.shadow[n] = self.shadow[n].to(p.device)
                p.data.copy_(self.shadow[n])


class GradReverse(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, lambd):
        ctx.lambd = lambd
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.lambd, None


def grad_reverse(x, lambd: float):
    return GradReverse.apply(x, lambd)


# --------------------------
# Dataset
# --------------------------
class EEGNPZDataset(Dataset):
    def __init__(self, roots: List[str], drop_artifacts=True, sample_rate=256, subjects_per_dataset=None,
                 subjects_allowlist=None):
        self.files = []
        tmp_files = []
        for root in roots:
            dname = os.path.basename(root)
            for p in pathlib.Path(root).rglob('*.npz'):
                try:
                    d = np.load(p, allow_pickle=True)
                    subj = str(d.get('subject', 'unknown'))
                except Exception:
                    subj = 'unknown'
                tmp_files.append((str(p), dname, subj))
        if len(tmp_files) == 0:
            raise AssertionError('No .npz files found.')
        if subjects_allowlist:
            allow = {ds: set(subjs) for ds, subjs in subjects_allowlist.items()}
            self.files = [(p, ds, s) for (p, ds, s) in tmp_files if ds in allow and (not allow[ds] or s in allow[ds])]
        elif subjects_per_dataset:
            picked = {}
            for p, ds, s in tmp_files:
                picked.setdefault(ds, set())
                if len(picked[ds]) < subjects_per_dataset or s in picked[ds]:
                    self.files.append((p, ds, s))
                    picked[ds].add(s)
        else:
            self.files = tmp_files

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        path, dataset_id, subject = self.files[idx]
        d = np.load(path, allow_pickle=True)
        x = d['x'].astype(np.float32)
        mask_missing = d['mask_missing'].astype(np.float32)
        return {'x_raw': x, 'mask_missing': mask_missing, 'dataset_id': dataset_id, 'subject': subject}


# --------------------------
# Model components (dummy encoders)
# --------------------------
class DummyEFMRawEncoder(nn.Module):
    def __init__(self, out_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv1d(19, 64, 7, padding=3), nn.GELU(),
            nn.Conv1d(64, 128, 7, padding=3), nn.GELU(),
            nn.AdaptiveAvgPool1d(1), nn.Flatten(), nn.Linear(128, out_dim)
        )

    def forward(self, x): return self.net(x)


class DummyEFMSpecEncoder(nn.Module):
    def __init__(self, in_ch=19, out_dim=512):
        super().__init__()
        self.net = nn.Sequential(
            nn.Conv2d(in_ch, 64, 3, padding=1), nn.GELU(),
            nn.Conv2d(64, 128, 3, padding=1), nn.GELU(),
            nn.AdaptiveAvgPool2d(1), nn.Flatten(), nn.Linear(128, out_dim)
        )

    def forward(self, x): return self.net(x)


class DummyFusion(nn.Module):
    def __init__(self, dim=512):
        super().__init__()
        self.fc = nn.Linear(dim * 2, dim)

    def forward(self, r, s): return self.fc(torch.cat([r, s], dim=-1))


# --------------------------
# STFT + Augmentations
# --------------------------
class SimpleLogSTFT(nn.Module):
    def __init__(self, n_fft: int = 256, hop: int = 128, eps: float = 1e-6):
        super().__init__()
        self.n_fft, self.hop, self.eps = n_fft, hop, eps

    def forward(self, x: torch.Tensor):
        B, C, T = x.shape
        specs = []
        for c in range(C):
            S = torch.stft(x[:, c, :], n_fft=self.n_fft, hop_length=self.hop, return_complex=True)
            P = (S.real ** 2 + S.imag ** 2).clamp_min(self.eps)
            specs.append(torch.log(P))  # (B,F,TT)
        return torch.stack(specs, dim=1)  # (B,C,F,TT)


class RawAug(nn.Module):
    def __init__(self, time_jitter: int = 64, noise_sigma: float = 0.02, channel_drop_p: float = 0.1,
                 time_warp_pct: float = 0.05):
        super().__init__()
        self.time_jitter = time_jitter
        self.noise_sigma = noise_sigma
        self.channel_drop_p = channel_drop_p
        self.time_warp_pct = time_warp_pct

    def forward(self, x: torch.Tensor, mask_missing: torch.Tensor):
        B, C, T = x.shape
        tj = int(self.time_jitter)
        shift = torch.randint(low=-tj, high=tj + 1, size=(B,), device=x.device)
        x2 = torch.zeros_like(x)
        for b in range(B):
            s = shift[b].item()
            if s >= 0:
                x2[b, :, s:] = x[b, :, :T - s]
            else:
                x2[b, :, :T + s] = x[b, :, -s:]
        x2 = x2 + self.noise_sigma * torch.randn_like(x2)
        drop = (torch.rand(B, C, 1, device=x.device) < self.channel_drop_p).float()
        mm = mask_missing.unsqueeze(-1) if mask_missing.ndim == 2 else mask_missing
        x2 = x2 * (1.0 - drop) * (1.0 - mm) + x2 * (1.0 - mm)
        warp = 1.0 + (2 * torch.rand(B, device=x.device) - 1.0) * self.time_warp_pct
        out = torch.zeros_like(x2)
        grid = torch.linspace(0, 1, T, device=x.device)
        for b in range(B):
            t_new = (grid * warp[b]).clamp(0, 1)
            idx = (t_new * (T - 1)).round().long()
            out[b] = x2[b, :, idx]
        return out


class SpecAug(nn.Module):
    def __init__(self, time_mask_pct: float = 0.1, freq_mask_pct: float = 0.1):
        super().__init__()
        self.tmp = time_mask_pct
        self.fmp = freq_mask_pct

    def forward(self, spec: torch.Tensor):
        B, C, F, T = spec.shape
        out = spec.clone()
        tlen = int(T * self.tmp)
        if tlen > 0:
            t0 = torch.randint(0, max(1, T - tlen + 1), (B,), device=spec.device)
            for b in range(B):
                out[b, :, :, t0[b]:t0[b] + tlen] = 0
        flen = int(F * self.fmp)
        if flen > 0:
            f0 = torch.randint(0, max(1, F - flen + 1), (B,), device=spec.device)
            for b in range(B):
                out[b, :, f0[b]:f0[b] + flen, :] = 0
        return out


# --------------------------
# Heads & Queues
# --------------------------
class ProjHead(nn.Module):
    def __init__(self, in_dim: int, proj_dim: int = 256):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(in_dim, in_dim), nn.GELU(), nn.Linear(in_dim, proj_dim))

    def forward(self, x): return F.normalize(self.net(x), dim=-1)


class JigsawHead(nn.Module):
    def __init__(self, in_dim: int, num_perm: int = 24):
        super().__init__()
        self.fc = nn.Linear(in_dim, num_perm)

    def forward(self, x): return self.fc(x)


class DomainHead(nn.Module):
    def __init__(self, in_dim: int, num_domains: int):
        super().__init__()
        self.fc = nn.Linear(in_dim, num_domains)

    def forward(self, x, lambd: float): return self.fc(grad_reverse(x, lambd))


class MemoryQueue:
    def __init__(self, dim: int, size: int = 65536, device: str = 'cuda'):
        self.size = size;
        self.ptr = 0;
        self.queue = F.normalize(torch.randn(size, dim, device=device), dim=-1)

    @torch.no_grad()
    def enqueue(self, feats: torch.Tensor):
        n = feats.shape[0]
        if n >= self.size:
            self.queue = feats[-self.size:].detach();
            self.ptr = 0;
            return
        end = self.ptr + n
        if end <= self.size:
            self.queue[self.ptr:end] = feats.detach()
        else:
            first = self.size - self.ptr
            self.queue[self.ptr:] = feats[:first].detach()
            self.queue[:end - self.size] = feats[first:].detach()
        self.ptr = (self.ptr + n) % self.size


def info_nce(q: torch.Tensor, k: torch.Tensor, queue: MemoryQueue, temperature: float = 0.07):
    q = F.normalize(q, dim=-1);
    k = F.normalize(k, dim=-1)
    pos = torch.sum(q * k, dim=-1, keepdim=True)
    neg = q @ queue.queue.t()
    logits = torch.cat([pos, neg], dim=1) / temperature
    labels = torch.zeros(q.size(0), dtype=torch.long, device=q.device)
    return F.cross_entropy(logits, labels)


class SubjectProtoBank(nn.Module):
    def __init__(self, dim: int = 512, momentum: float = 0.9):
        super().__init__()
        self.dim = dim;
        self.m = momentum
        self.register_buffer('keys', torch.empty(0, dim))
        self.subject_to_idx: Dict[str, int] = {}

    @torch.no_grad()
    def _ensure_subject(self, subject: List[str], device):
        for s in subject:
            if s not in self.subject_to_idx:
                self.subject_to_idx[s] = len(self.subject_to_idx)
                new_key = F.normalize(torch.randn(1, self.dim, device=device), dim=-1)
                self.keys = new_key if self.keys.numel() == 0 else torch.cat([self.keys, new_key], dim=0)

    @torch.no_grad()
    def update(self, subjects: List[str], feats: torch.Tensor):
        self._ensure_subject(subjects, feats.device)
        for i, s in enumerate(subjects):
            idx = self.subject_to_idx[s]
            k = self.keys[idx]
            k.mul_(self.m).add_(F.normalize(feats[i], dim=-1), alpha=1.0 - self.m)
            self.keys[idx] = F.normalize(k, dim=-1)

    def loss(self, subjects: List[str], feats: torch.Tensor, temperature: float = 0.07):
        self._ensure_subject(subjects, feats.device)
        feats = F.normalize(feats, dim=-1)
        protos = F.normalize(self.keys, dim=-1)
        logits = feats @ protos.t() / temperature
        idxs = torch.tensor([self.subject_to_idx[s] for s in subjects], device=feats.device, dtype=torch.long)
        return F.cross_entropy(logits, idxs)


# --------------------------
# EFM SSL wrapper
# --------------------------
class EFM_SSL(nn.Module):
    def __init__(self, proj_dim=256, num_domains=3, jigsaw_perms=24, ema_m=0.996, queue_size=65536, proto_m=0.9):
        super().__init__()
        self.raw_enc = DummyEFMRawEncoder(512)
        self.spec_enc = DummyEFMSpecEncoder(19, 512)
        self.fusion = DummyFusion(512)
        self.raw_proj = ProjHead(512, proj_dim)
        self.spec_proj = ProjHead(512, proj_dim)
        self.jigsaw_head = JigsawHead(512, jigsaw_perms)
        self.domain_head = DomainHead(512, num_domains)
        self.raw_ema = EMA(self.raw_enc, ema_m)
        self.spec_ema = EMA(self.spec_enc, ema_m)
        self.queue = MemoryQueue(proj_dim, size=queue_size, device='cuda' if torch.cuda.is_available() else 'cpu')
        self.proto = SubjectProtoBank(dim=512, momentum=proto_m)

    def forward_raw_latent(self, x): return self.raw_enc(x)

    def forward_spec_latent(self, spec): return self.spec_enc(spec)


# --------------------------
# SSLConfig
# --------------------------
@dataclass
class SSLConfig:
    data_roots: List[str] = None
    subjects_per_dataset: Optional[int] = None
    subjects_allowlist_path: Optional[str] = None
    batch_size: int = 32
    num_workers: int = 8
    lr: float = 3e-4
    total_steps: int = 200000
    out_dir: str = 'runs/ssl/efm_ssl_base'


# --------------------------
# Helpers
# --------------------------
def cosine_lr(step, total, base_lr, warmup):
    if step < warmup: return base_lr * step / max(1, warmup)
    s = (step - warmup) / max(1, total - warmup)
    return 0.5 * base_lr * (1 + math.cos(math.pi * s))


def masked_eeg_targets(x: torch.Tensor, mask_ratio_min: float, mask_ratio_max: float, tmin: int, tmax: int):
    B, C, T = x.shape
    ratio = torch.empty(B, device=x.device).uniform_(mask_ratio_min, mask_ratio_max)
    mask = torch.zeros(B, C, T, device=x.device, dtype=torch.bool)
    for b in range(B):
        total = int(ratio[b].item() * C * T)
        covered = 0
        while covered < total:
            span = random.randint(tmin, tmax)
            c = random.randint(0, C - 1)
            t0 = random.randint(0, max(0, T - span))
            mask[b, c, t0:t0 + span] = True
            covered += span
    return mask


def make_domain_index(names: List[str]) -> Dict[str, int]:
    uniq = sorted(set(names))
    return {n: i for i, n in enumerate(uniq)}


# --------------------------
# Train
# --------------------------
def train(cfg: SSLConfig):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    set_seed(42)

    # Data
    allow = None
    if cfg.subjects_allowlist_path and os.path.exists(cfg.subjects_allowlist_path):
        with open(cfg.subjects_allowlist_path) as f:
            allow = json.load(f)
    ds = EEGNPZDataset(cfg.data_roots, subjects_per_dataset=cfg.subjects_per_dataset, subjects_allowlist=allow)
    domain_map = make_domain_index([dsname for (_, dsname, _) in ds.files])
    loader = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers,
                        pin_memory=True, drop_last=True)
    print(f"Dataset loaded with {len(ds)} samples.")

    # Model & optim
    model = EFM_SSL(proj_dim=256, num_domains=len(domain_map), ema_m=0.996, queue_size=65536, proto_m=0.9).to(device)
    raw_aug, stft, spec_aug = RawAug(), SimpleLogSTFT(), SpecAug()
    opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=0.05)
    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())

    os.makedirs(cfg.out_dir, exist_ok=True)
    step, t0 = 0, time.time()
    total_steps, warmup_steps = cfg.total_steps, max(1, int(0.01 * cfg.total_steps))

    while step < total_steps:
        for batch in loader:
            if step >= total_steps: break
            x = torch.as_tensor(batch['x_raw']).to(device)
            mm = torch.as_tensor(batch['mask_missing']).to(device)
            subjects = batch['subject']
            dnames = batch['dataset_id']
            d_idx = torch.tensor([domain_map[n] for n in dnames], device=device)

            # Multi-view
            x_v1 = raw_aug(x, mm);
            x_v2 = raw_aug(x, mm)
            spec = spec_aug(stft(x))
            _ = masked_eeg_targets(x_v1, 0.30, 0.60, 64, 256)  # placeholder for MEM

            opt.zero_grad(set_to_none=True)
            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):
                # Student latents
                r_lat_v1 = model.forward_raw_latent(x_v1)
                r_lat_v2 = model.forward_raw_latent(x_v2)
                s_lat = model.forward_spec_latent(spec)
                q_raw = model.raw_proj(r_lat_v1)
                q_spec = model.spec_proj(s_lat)

                # EMA teachers
                model.raw_ema.update(model.raw_enc);
                model.spec_ema.update(model.spec_enc)
                with torch.no_grad():
                    r_lat_v2_t = model.forward_raw_latent(x_v2).detach()
                    s_lat_t = model.forward_spec_latent(spec).detach()
                    k_raw = F.normalize(model.raw_proj(r_lat_v2_t), dim=-1)
                    k_spec = F.normalize(model.spec_proj(s_lat_t), dim=-1)

                # Losses
                L_contrast = info_nce(q_raw, k_spec, model.queue) + info_nce(q_spec, k_raw, model.queue)
                L_mem = F.mse_loss(r_lat_v1, r_lat_v2_t)
                perm_logits = model.jigsaw_head(r_lat_v1)
                perm_labels = torch.randint(0, perm_logits.size(1), (x.size(0),), device=device)
                L_jig = F.cross_entropy(perm_logits, perm_labels)
                fusion_lat = model.fusion(r_lat_v1, s_lat)
                lambd = min(1.0, step / max(1, int(total_steps * 0.3)))
                dom_logits = model.domain_head(fusion_lat.detach(), lambd)
                L_grl = F.cross_entropy(dom_logits, d_idx)
                L_proto = model.proto.loss(subjects, fusion_lat)
                loss = (1.0 * L_mem + 1.0 * L_contrast + 0.5 * L_jig + 0.2 * L_grl + 0.5 * L_proto)

            scaler.scale(loss).backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            lr_now = cosine_lr(step, total_steps, cfg.lr, warmup_steps)
            for g in opt.param_groups: g['lr'] = lr_now
            scaler.step(opt);
            scaler.update()

            with torch.no_grad():
                model.queue.enqueue(k_raw);
                model.queue.enqueue(k_spec)
                model.proto.update(subjects, fusion_lat)

            if (step % 100) == 0:
                print(json.dumps(
                    {'step': step, 'lr': lr_now, 'loss': float(loss.item()), 'time_s': round(time.time() - t0, 2)}))
            if (step % 10000) == 0 and step > 0:
                torch.save({'model': model.state_dict()}, os.path.join(cfg.out_dir, f'ssl_step{step}.ckpt'))

            step += 1

    torch.save({'model': model.state_dict()}, os.path.join(cfg.out_dir, 'efm_base.ckpt'))


# --------------------------
# Argparse
# --------------------------
def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('--config', type=str, default=None)
    p.add_argument('--data-roots', nargs='+', dest='data_roots', required=True)
    p.add_argument('--limit-subjects-per-dataset', dest='limit_subjects_per_dataset', type=int, default=None)
    p.add_argument('--subjects-allowlist', dest='subjects_allowlist', type=str, default=None)
    p.add_argument('--batch-size', dest='batch_size', type=int, default=32)
    p.add_argument('--out', dest='out', type=str, default='runs/ssl/efm_ssl_base')
    args = p.parse_args()
    cfg = SSLConfig()
    # optional YAML: we accept but don't require
    if args.config and yaml is not None and os.path.exists(args.config):
        try:
            with open(args.config, 'r') as f:
                y = yaml.safe_load(f)
            if isinstance(y, dict):
                t = y.get('train', {})
                cfg.lr = t.get('base_lr', cfg.lr)
                cfg.total_steps = t.get('total_steps', cfg.total_steps)
        except Exception:
            pass
    cfg.data_roots = args.data_roots
    cfg.subjects_per_dataset = args.limit_subjects_per_dataset
    cfg.subjects_allowlist_path = args.subjects_allowlist
    cfg.batch_size = args.batch_size
    cfg.out_dir = args.out
    return cfg


if __name__ == '__main__':
    cfg = parse_args()
    print('Config:', cfg)
    train(cfg)
